{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Thesis Research Project Code**\n",
    "### Independent Virtual Reality Training on Public Speaking Skills of University Students: An Experimental Study\n",
    "---\n",
    "\n",
    "Michael Matheo Kallas - i6283098\n",
    "\n",
    "ðŸ“§ m.kallas@student.maastrichtuniversity.nl\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import syllapy\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Means of Moving Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_moving_median(file_path, window_size):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Time'] = pd.to_datetime(data['Time (ms)'], unit='ms')\n",
    "    data.set_index('Time', inplace=True)\n",
    "    data['Moving Median'] = data['Skin_Conductance (uS)'].rolling(window=window_size).median()\n",
    "    mean_of_moving_median = data['Moving Median'].mean()\n",
    "    data['Time Numeric'] = (data.index - data.index[0]).total_seconds() \n",
    "    return mean_of_moving_median\n",
    "\n",
    "results_df = pd.DataFrame(columns=['File', 'Mean of Moving Median'])\n",
    "window_size = '3000ms'\n",
    "\n",
    "for id_number in range(1, 25):  \n",
    "    for trial in ['G1', 'G2']:  \n",
    "        file_name = f'{trial}{id_number:02}.csv'\n",
    "        mean_of_moving_median = compute_moving_median(file_name, window_size)\n",
    "        new_row = pd.DataFrame({\n",
    "            'File': [file_name], \n",
    "            'Mean of Moving Median': [mean_of_moving_median],\n",
    "        })\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "results_df.to_excel('GSR_Mean.xlsx', index=False)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocal Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f0 Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marking IPU Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_data = []\n",
    "\n",
    "for i in range(1, 25):\n",
    "    for identifier in ['M1', 'M2']:\n",
    "        file_number = f'{i:02}'\n",
    "        audio_file = f'{file_number}{identifier}.wav'\n",
    "        signal, sampling_rate = librosa.load(audio_file)\n",
    "        intensity_envelope = librosa.feature.rms(y=signal)[0]\n",
    "        intensity_db = librosa.amplitude_to_db(intensity_envelope, ref=np.max)\n",
    "        times = librosa.frames_to_time(np.arange(len(intensity_db)), sr=sampling_rate)\n",
    "        silent_gaps = intensity_db < -25\n",
    "        gap_indices = np.where(silent_gaps)[0]\n",
    "        gap_regions = np.split(gap_indices, np.where(np.diff(gap_indices) != 1)[0] + 1)\n",
    "        for gap in gap_regions:\n",
    "            if gap.size > 0:\n",
    "                start_time = times[gap[0]]\n",
    "                end_time = times[gap[-1]]\n",
    "                gap_duration = end_time - start_time\n",
    "                if gap_duration >= 0.2:  \n",
    "                    gaps_data.append({'File': audio_file, 'Start Time': start_time, 'End Time': end_time})\n",
    "\n",
    "silent_gaps_df = pd.DataFrame(gaps_data)\n",
    "silent_gaps_df.to_csv('silent_gaps.csv', index=False)\n",
    "display(silent_gaps_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering f0 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_f0_data(file_path, silent_gaps):\n",
    "    f0_data = pd.read_csv(file_path, delim_whitespace=True)\n",
    "    keep_mask = pd.Series(True, index=f0_data.index)\n",
    "    for _, row in silent_gaps.iterrows():\n",
    "        start_time = row['Start Time']\n",
    "        end_time = row['End Time']\n",
    "        keep_mask = keep_mask & ((f0_data['Time'] < start_time) | (f0_data['Time'] > end_time))\n",
    "    filtered_data = f0_data[keep_mask]\n",
    "    filtered_data.to_csv(file_path.replace('.f0', '_filtered.txt'), sep='\\t')\n",
    "\n",
    "for i in range(1, 25):\n",
    "    for identifier in ['M1', 'M2']:\n",
    "        file_number = f'{i:02}'\n",
    "        audio_file = f'{file_number}{identifier}'\n",
    "        file_path = f'{audio_file}.f0'\n",
    "        file_specific_gaps = silent_gaps_df[silent_gaps_df['File'].str.contains(audio_file)]        \n",
    "        filter_f0_data(file_path, file_specific_gaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating f0 Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hz_to_semitones(f0_values):\n",
    "    semitones = 12 * np.log2(f0_values / 100)\n",
    "    return semitones\n",
    "\n",
    "def calculate_f0_stats(file_path):\n",
    "    f0_data = pd.read_csv(file_path, delim_whitespace=True)\n",
    "    f0_data['Semitones'] = hz_to_semitones(f0_data['F0'])\n",
    "    stats = {\n",
    "        'Mean Semitones': f0_data['Semitones'].mean(),\n",
    "        'STD Semitones': f0_data['Semitones'].std()\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "filtered_data = [f for f in os.listdir() if '_filtered.txt' in f]\n",
    "results = []\n",
    "\n",
    "for file in filtered_data:\n",
    "    file_stats = calculate_f0_stats(file)\n",
    "    if file_stats:\n",
    "        file_stats['File'] = file\n",
    "        results.append(file_stats)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel('f0_statistics.xlsx', index=False)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tempo Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting Silence Durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_silences(audio_file):\n",
    "    signal, sampling_rate = librosa.load(audio_file)\n",
    "    intensity_envelope = librosa.feature.rms(y=signal)[0]\n",
    "    intensity_db = librosa.amplitude_to_db(intensity_envelope, ref=np.max)\n",
    "    times = librosa.frames_to_time(np.arange(len(intensity_db)), sr=sampling_rate)\n",
    "    silent_gaps = intensity_db < - 25\n",
    "    gap_indices = np.where(silent_gaps)[0]\n",
    "    if len(gap_indices) == 0:\n",
    "        return 0.0  \n",
    "    gap_regions = np.split(gap_indices, np.where(np.diff(gap_indices) != 1)[0] + 1)\n",
    "    total_silence_duration = 0.0\n",
    "    for gap in gap_regions:\n",
    "        if len(gap) > 0:\n",
    "            start_time = times[gap[0]]\n",
    "            end_time = times[gap[-1]]\n",
    "            gap_duration = end_time - start_time\n",
    "            if gap_duration >= 0.3:\n",
    "                total_silence_duration += gap_duration\n",
    "    return total_silence_duration\n",
    "\n",
    "audio_directory = os.getcwd()\n",
    "audio_files = []\n",
    "for i in range(1, 25):\n",
    "    for m in ['M1', 'M2']:\n",
    "        audio_files.append(f'{i:02d}{m}.wav')\n",
    "results = []\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    file_path = os.path.join(audio_directory, audio_file)\n",
    "    if os.path.isfile(file_path):\n",
    "        silence_duration = detect_silences(file_path)\n",
    "        results.append({\n",
    "            'Filename': audio_file,\n",
    "            'Total Silence Duration': silence_duration\n",
    "        })\n",
    "\n",
    "silences_df = pd.DataFrame(results)\n",
    "display(silences_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcribing Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(file_path):\n",
    "    with sr.AudioFile(file_path) as source:\n",
    "        audio = sr.Recognizer().record(source) \n",
    "        return sr.Recognizer().recognize_google(audio)\n",
    "\n",
    "directory = os.getcwd()\n",
    "\n",
    "transcript_file = 'transcripts.txt'\n",
    "with open(transcript_file, 'w', encoding='utf-8') as file_out:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            transcript = transcribe_audio(file_path)\n",
    "            file_out.write(f'Filename: {filename}\\nTranscript: {transcript}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Number of Syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transcripts(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().split('Filename: ')\n",
    "    transcripts = {}\n",
    "    for entry in content[1:]:  \n",
    "        lines = entry.strip().split('\\n')\n",
    "        filename = lines[0].strip()  \n",
    "        transcript = ' '.join(lines[1:])  \n",
    "        transcripts[filename] = transcript.split('Transcript: ')[1].strip()\n",
    "    return transcripts\n",
    "\n",
    "def count_total_syllables(transcripts):\n",
    "    syllable_counts = []\n",
    "    for filename, transcript in transcripts.items():\n",
    "        syllable_count = 0\n",
    "        words = transcript.split()\n",
    "        for word in words:\n",
    "            syllable_count += syllapy.count(word.lower())\n",
    "        syllable_counts.append({'Filename': filename, 'Total Syllables': syllable_count})\n",
    "    return syllable_counts\n",
    "\n",
    "file_path = 'transcripts.txt'\n",
    "transcripts = extract_transcripts(file_path)\n",
    "syllable_data = count_total_syllables(transcripts)\n",
    "syllable_df = pd.DataFrame(syllable_data)\n",
    "display(syllable_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Tempo Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tempo_stats(file, silences_df, syllable_df):\n",
    "    audio_file = os.path.join(audio_directory, file)\n",
    "    signal, sampling_rate = librosa.load(audio_file)\n",
    "    total_time_of_presentation = min(librosa.get_duration(y=signal, sr=sampling_rate), 120)\n",
    "    silence_info = silences_df[silences_df['Filename'] == file]\n",
    "    syllable_info = syllable_df[syllable_df['Filename'] == file]\n",
    "    total_silence_duration = silence_info['Total Silence Duration'].values[0]\n",
    "    total_syllables = syllable_info['Total Syllables'].values[0]\n",
    "    total_speaking_time = total_time_of_presentation - total_silence_duration\n",
    "    speech_rate = total_syllables / total_time_of_presentation if total_time_of_presentation > 0 else 0\n",
    "    asd = total_speaking_time / total_syllables if total_syllables > 0 else 0\n",
    "    return {\n",
    "        'Filename': file,\n",
    "        'Presentation Time': total_time_of_presentation,\n",
    "        'Speech Rate': speech_rate,\n",
    "        'ASD': asd\n",
    "    }\n",
    "\n",
    "audio_directory = os.getcwd()\n",
    "files = []\n",
    "for i in range(1, 25):\n",
    "    for m in ['M1', 'M2']:\n",
    "        files.append(f'{i:02d}{m}.wav')\n",
    "results = [calculate_tempo_stats(filename, silences_df, syllable_df) for filename in files]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel('tempo_statistics.xlsx', index=False)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
